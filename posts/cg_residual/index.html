<!doctype html><html lang=en><head><title>Behavior of conjugate gradient residual | Gabriel H. Brown</title>
<meta name=viewport content="width=device-width,initial-scale=1"><meta charset=UTF-8><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css integrity="sha512-1sCRPdkRXhBV2PBLUdRb4tMg1w2YPf37qatUFeS7zlBy7jJI8Lf4VHwWfZZfpXtYSLy85pkm9GaYVYMfw5BC1A==" crossorigin=anonymous><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/academicons/1.9.1/css/academicons.min.css integrity="sha512-b1ASx0WHgVFL5ZQhTgiPWX+68KjS38Jk87jg7pe+qC7q9YkEtFq0z7xCglv7qGIs/68d3mAp+StfC8WKC5SSAg==" crossorigin=anonymous><link rel=stylesheet href=https://ghbrown.net/css/palettes/color-splotch.css><link rel=stylesheet href=https://ghbrown.net/css/risotto.css><link rel=stylesheet href=https://ghbrown.net/css/custom.css><script src=https://ghbrown.net/assets/quotes.js></script><script src=https://ghbrown.net/js/serve_quote.js></script><script type=text/javascript src=//code.jquery.com/jquery-latest.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body><div class=page><header class=page__header><nav class="page__nav main-nav"><ul><h1 class=page__logo><a href=https://ghbrown.net/ class=page__logo-inner>Gabriel H. Brown</a></h1><li class=main-nav__item><a class=nav-main-item href=https://ghbrown.net/ title>About Me</a></li><li class=main-nav__item><a class=nav-main-item href=https://ghbrown.net/topics/research title>Research</a></li><li class=main-nav__item><a class=nav-main-item href=https://ghbrown.net/topics/publications title>Publications</a></li><li class=main-nav__item><a class=nav-main-item href=https://ghbrown.net/topics/open_source title>Open Source</a></li><li class=main-nav__item><a class=nav-main-item href=https://ghbrown.net/topics/hobbies_and_interests title>Hobbies</a></li><li class=main-nav__item><a class=nav-main-item href=https://ghbrown.net/tags title>Post Topics</a></li></ul></nav></header><section class=page__body><div class=content__body><h1 id=behavior-of-the-conjugate-gradient-residual>Behavior of the conjugate gradient residual</h1><hr><p>An often stressed property of the conjugate gradient method (CG) for solving linear systems is the monotonic decrease in the A-norm of the error.
When CG is applied in practice the exact solution is unknown and the error cannot be computed or tracked, so the residual or relative residual is used instead.</p><p>However, the kinds of guarantees available for residuals are slightly weaker and more involved than is often presented, due to complications introduced by different norms and arithmetic precisions.
Per
<a href=https://www.scirp.org/html/2644.html class=mainlink target=_blank><span class=themecolor>an article by Washizawa</span></a> we have</p><ul><li>monotonic decrease of error in the A-norm in finite precision and exact arithmetic
\( \left( k > j \implies ||\mathbf{e}_k||_A \le ||\mathbf{e}_j||_A \right) \)</li><li>almost monotonic decrease of error in the 2-norm in exact arithmetic
\( \left( \exist k > j : ||\mathbf{e}_k||_2 \le ||\mathbf{e}_j||_2 \right) \)</li><li>almost monotonic decrease of residual in the 2-norm in finite precision and exact arithmetic
\( \left( \exist k > j : ||\mathbf{r}_k||_2 \le ||\mathbf{r}_j||_2 \right) \)</li></ul><p>Examples with increasing residual 2-norms are not difficult to come by in practice, for an example see slide 21 of
<a href=https://see.stanford.edu/materials/lsocoee364b/11-conj_grad_slides.pdf class=mainlink target=_blank><span class=themecolor>these slides by Stephen Boyd</span></a>.
However, I am unaware of any simple, tangible, and easily understandable examples where CG produces a temporarily increasing residual.
While attempting to find such a system I was able to prove something small about the behavior of the residual in the 2-norm for CG.
That is <strong>the 2-norm of the residual always decreases on the first iteration</strong>
$$
||\mathbf{r}_1||_2 \lt ||\mathbf{r}_0||_2
\quad \forall \; \mathbf{A}, \mathbf{b}, \mathbf{x}_0 .
$$</p><details><summary>Proof</summary><p>We wish to find a system whose residual does not decrease in the 2-norm on the first iteration, that is \( ||\mathbf{r}_1||_2 \ge ||\mathbf{r}_0||_2 \).</p><p>Using the standard conjugate gradient iteration pseudocode (from Trefethen and Bau, for example) one gets
$$
\mathbf{r}_0 = \mathbf{A}\mathbf{x}_0 - \mathbf{b}, \quad\quad
\mathbf{r}_1 = \mathbf{r}_0 -
\frac{\mathbf{r}_0^T\mathbf{r}_0}{\mathbf{r}_0^T \mathbf{A} \mathbf{r}_0}
\mathbf{A} \mathbf{r}_0 .
$$
Satisfying \( ||\mathbf{r}_1||_2 \ge ||\mathbf{r}_0||_2 \) is equivalent to
$$
\left( \mathbf{r}_0 -
\frac{\mathbf{r}_0^T\mathbf{r}_0}{\mathbf{r}_0^T \mathbf{A} \mathbf{r}_0}
\mathbf{A} \mathbf{r}_0 \right)^T
\left( \mathbf{r}_0 -
\frac{\mathbf{r}_0^T\mathbf{r}_0}{\mathbf{r}_0^T \mathbf{A} \mathbf{r}_0}
\mathbf{A} \mathbf{r}_0 \right)
\ge
\mathbf{r}_0^T \mathbf{r}_0
$$
or in a more simplified form in terms of normed quantities
$$
\frac{||\mathbf{r}_0||_2^4}{\left(\mathbf{r}_0^T \mathbf{A} \mathbf{r}_0\right)^2}
||\mathbf{A}\mathbf{r}_0||_2^2 - 2 ||\mathbf{r}_0||_2^2 \ge 0 .
$$
Further simplification yields the equivalent statement
$$
||\mathbf{r}_0||_2 ||\mathbf{A}\mathbf{r}_0||_2
\ge
\sqrt{2} \mathbf{r}_0^T \mathbf{A} \mathbf{r}_0.
$$</p><p>Now, take advantage of the symmetric positive definiteness of \( \mathbf{A} \) to insert the eigendecomposition \( \mathbf{A} = \mathbf{X}^T \mathbf{\Lambda X} \) with unitary \( \mathbf{X} \) and diagonal, positive \( \mathbf{\Lambda} \).
Further, define the new variable \( \mathbf{v} = \mathbf{Xr}_0 = ||\mathbf{r}_0||_2 \hat{\mathbf{v}} \) (where \( ||\hat{\mathbf{v}}||_2 = 1 \)) to simplify to the equivalent inequality
$$
||\mathbf{\Lambda} \hat{\mathbf{v}}||_2
\ge
\sqrt{2} \hat{\mathbf{v}}^T \mathbf{\Lambda} \hat{\mathbf{v}} . \quad (*)
$$
Taking the negation of this statement, that is
$$
\sqrt{2} \hat{\mathbf{v}}^T \mathbf{\Lambda} \hat{\mathbf{v}}
\gt
||\mathbf{\Lambda} \hat{\mathbf{v}}||_2 \quad \text{(negation)} ,
$$
and employing the Cauchy-Schwarz inequality for the left term (using \( ||\hat{\mathbf{v}}||_2 = 1 \)) one gets
$$
\sqrt{2} \hat{\mathbf{v}}^T \mathbf{\Lambda} \hat{\mathbf{v}}
\ge
\sqrt{2} ||\mathbf{\Lambda} \hat{\mathbf{v}}||_2
\gt
||\mathbf{\Lambda} \hat{\mathbf{v}}||_2 \quad \text{(negation)} .
$$
Since the right of these two inequalities is true for arbitrary \( \Lambda, \; \hat{\mathbf{v}} \) then
$$
\sqrt{2} \hat{\mathbf{v}}^T \mathbf{\Lambda} \hat{\mathbf{v}}
\gt
||\mathbf{\Lambda} \hat{\mathbf{v}}||_2 \quad \text{(negation)}
$$
is also true for all \( \Lambda, \; \hat{\mathbf{v}} \).</p><p>This being the negation, we have proved \( (*) \) false generically, which is equivalent to
$$
||\mathbf{r}_1||_2 \ge ||\mathbf{r}_0||_2 \;\; \text{false} \quad
\forall \mathbf{A}, \; \mathbf{b}, \; \mathbf{x}_0 .
$$
In more direct language
$$
||\mathbf{r}_1||_2 \le ||\mathbf{r}_0||_2 \quad
\forall \mathbf{A}, \; \mathbf{b}, \; \mathbf{x}_0 ,
$$
completing the proof.</p></details><hr><h3 id=future-work-and-comments>Future work and comments</h3><p>I&rsquo;d like to determine if this result is also true in finite precision, and find an increasing system if it is false in finite precision.
Furthermore, since we have almost monotonic decrease of residual 2-norm in exact arithmetic, I&rsquo;d like to determine whether the residual could increase on the second iteration.</p><p>Comment 1: I think the Washizawa paper has some errors, see for example Equation 20, where it should be \( || \overline{\mathbf{r}_i} - \varepsilon_M(\mathbf{r}_i)|| \) (one less bar).</p><p>Comment 2: I&rsquo;d like to post about this
<a href=https://epubs.siam.org/doi/abs/10.1137/S0895479894275030 class=mainlink target=_blank><span class=themecolor>amazing result by Greenbaum</span></a> at some point, perhaps in a separate post, and after I&rsquo;ve had time to read it.</p><p>Comment 3: I&rsquo;d also like to discuss the common misconception about conjugate gradient and eigenvalue clusters brought up at 13:35 in
<a href="https://www.youtube.com/watch?v=jpBzZP2f5Wk" class=mainlink target=_blank><span class=themecolor>this wonderful talk by Zdenek Strakos</span></a>.</p></div><footer class=content__footer></footer></section><section class=page__aside><div class=aside__about><div id=quote><h1>A fun quote</h1></div><script>newQuote()</script></div><hr><div class=aside__content><p>posted 2022-11-29</p></div></section><footer class=page__footer><p></p><br><br><p class=copyright>Copyright <a href=/>Gabriel H. Brown</a>
.</p></footer></div></body></html>