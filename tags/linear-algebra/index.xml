<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>(Multi)linear Algebra on Gabriel H. Brown</title><link>https://ghbrown.net/tags/linear-algebra/</link><description>Recent content in (Multi)linear Algebra on Gabriel H. Brown</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Copyright [Gabriel H. Brown](/).</copyright><lastBuildDate>Sat, 06 Apr 2024 10:21:47 -0500</lastBuildDate><atom:link href="https://ghbrown.net/tags/linear-algebra/index.xml" rel="self" type="application/rss+xml"/><item><title>Finding a rank 1 matrix orthogonal to another matrix</title><link>https://ghbrown.net/posts/orthogonal_rank_1/</link><pubDate>Sat, 06 Apr 2024 10:21:47 -0500</pubDate><guid>https://ghbrown.net/posts/orthogonal_rank_1/</guid><description>Finding a rank 1 matrix orthogonal to another matrix Recently I needed to solve the following problem: given a square matrix $\mathbf{A}$ find another square matrix $\mathbf{B}$ satisfying $\text{vec}(\mathbf{A})^T \text{vec}(\mathbf{B}) = 0$. This is the sense in which I use the word orthogonal here, rather than the columns of $\mathbf{A}$ or $\mathbf{B}$ being orthogonal. Anyway, it&amp;rsquo;s impossible for the columns of a rank 1 matrix to be orthogonal.
If you are like me this problem sounds: easy at first glance, then quite difficult, then easy again.</description></item><item><title>A sufficient condition for non-negative solutions to non-negative linear systems</title><link>https://ghbrown.net/posts/sufficient_nonnegative/</link><pubDate>Fri, 29 Sep 2023 19:19:38 -0600</pubDate><guid>https://ghbrown.net/posts/sufficient_nonnegative/</guid><description>A sufficient condition for non-negative solutions to non-negative linear systems Here we are concerned with &amp;ldquo;non-negative&amp;rdquo; linear systems, that is, linear systems where $\mathbf{A}, \mathbf{b} \geq \mathbf{0}$ (elementwise). In particular, we give a sufficient condition for non-negativity of the solution $\mathbf{x}$ to $\mathbf{Ax} = \mathbf{b}$.
First, we discuss an intuition for the result. The columns of $\mathbf{A}$ form a linear cone; if a vector $\mathbf{c}$ (a right hand side to the linear system) is known to lie in the cone, then one can find a &amp;ldquo;subcone&amp;rdquo; containing $\mathbf{c}$.</description></item><item><title>The space of matrices where LU requires pivoting is almost full-dimensional</title><link>https://ghbrown.net/posts/dim_lu_pivot_needed/</link><pubDate>Tue, 06 Jun 2023 19:19:38 -0600</pubDate><guid>https://ghbrown.net/posts/dim_lu_pivot_needed/</guid><description>The space of matrices requiring pivoted LU factorization is almost full-dimensional The pivoted LU factorization of square matrices is the key routine for the direct solution of linear systems. All square, invertible matrices have a pivoted LU factorization of the form $\mathbf{PA} = \mathbf{LU}$, where $\mathbf{P}$ is a permutation matrix and $\mathbf{L}, \; \mathbf{U}$ are respectively upper and lower triangular.
However, not all square, invertible matrices have a factorization of the form $\mathbf{A} = \mathbf{LU}$ (an LU factorization).</description></item><item><title>Behavior of conjugate gradient residual</title><link>https://ghbrown.net/posts/cg_residual/</link><pubDate>Tue, 29 Nov 2022 22:19:38 -0600</pubDate><guid>https://ghbrown.net/posts/cg_residual/</guid><description>Behavior of the conjugate gradient residual An often stressed property of the conjugate gradient method (CG) for solving linear systems is the monotonic decrease in the A-norm of the error. When CG is applied in practice the exact solution is unknown and the error cannot be computed or tracked, so the residual or relative residual is used instead.
However, the kinds of guarantees available for residuals are slightly weaker and more involved than is often presented, due to complications introduced by different norms and arithmetic precisions.</description></item><item><title>Linear Algebra Pronunciation Guide</title><link>https://ghbrown.net/posts/linear_algebra_pronunciation/</link><pubDate>Thu, 17 Nov 2022 18:00:23 -0600</pubDate><guid>https://ghbrown.net/posts/linear_algebra_pronunciation/</guid><description>Linear Algebra Pronunciation Guide Like all fields of mathematics, linear algebra has many prominent figures whose names are non-trivial to pronounce in English (the lingua franca of science and mathematics).
This is further complicated by numerical linear algebra libraries which have inconsistent and unintuitive pronunciations.
Cholesky pronounced: ko - less - key (somewhat throaty ko, like kho) often mispronounced: chuh - less - key notes: this is somewhat muddled by that fact that Choleskly was paternally Polish and maternally French, and grew up in France, but I side with the Polish pronunciation (see references) Schur pronounced: shore often mispronounced: sherr (rhymes with her) Krylov pronounced: cree - lav, cree - luvf often mispronounced: cry - lawv BLAS pronounced: blawz (rhymes with paws), bloss (rhymes with gloss, perhaps slightly less common) often mispronounced: blass (rhymes with class) LAPACK pronounced: L - A - pack (syllabic rhyme with bell - may - pack) often mispronounced: luh - pack SCALAPACK pronounced: skay - luh - pack (syllabic rhyme with may - uh - pack) often mispronounced: anything else PETSC pronounced: pet - see often mispronounced: anything else References: Cholesky, BLAS and LAPACK, PETSc</description></item><item><title>Interior eigenvectors of symmetric matrices are saddle points</title><link>https://ghbrown.net/posts/symmetric_saddle_points/</link><pubDate>Fri, 29 Jul 2022 22:19:38 -0600</pubDate><guid>https://ghbrown.net/posts/symmetric_saddle_points/</guid><description>Interior eigenvectors of symmetric matrices are saddle points Eigenpairs of symmetric matrices are intimately related to optimization and critical points, with the eigenvectors being critical points of the Rayleigh quotient. In optimization settings, the type of critical point (minimum, maximum, saddle point) is an important feature in designing and understanding algorithms.
This write up characterizes the nature of all critical points (eigenvectors), with the main result being that all interior eigenvectors are saddle points.</description></item><item><title>Relationship between power iteration and gradient descent</title><link>https://ghbrown.net/posts/pow_iter_grad_desc/</link><pubDate>Sun, 29 May 2022 22:19:38 -0600</pubDate><guid>https://ghbrown.net/posts/pow_iter_grad_desc/</guid><description>The relationship between power iteration and gradient descent Although the majority of successful algorithms for the symmetric tensor eigenvalue problem use optimization techniques directly, there are a few notable algorithms that do not appear to be based on optimization. Rather, they more closely resemble power iteration and shifted power iteration.
In this write up we show that this is a false dichotomy, and that these methods are equivalent to very simple and classical optimization methods.</description></item></channel></rss>